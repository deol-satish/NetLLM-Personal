{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial - Save Model and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deols\\anaconda3\\envs\\urllc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\deols\\anaconda3\\envs\\urllc\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "from pprint import pprint\n",
    "from munch import Munch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from config import cfg\n",
    "from baseline_special.utils.utils import load_traces\n",
    "from baseline_special.utils.constants import BITRATE_LEVELS\n",
    "from plm_special.trainer import Trainer\n",
    "from plm_special.evaluate import evaluate_on_env\n",
    "from plm_special.test import test_on_env\n",
    "from plm_special.data.dataset import ExperienceDataset\n",
    "from plm_special.models.rl_policy import OfflineRLPolicy\n",
    "from plm_special.models.state_encoder import EncoderNetwork\n",
    "from plm_special.models.low_rank import peft_model\n",
    "from plm_special.utils.utils import set_random_seed\n",
    "from plm_special.utils.plm_utils import load_plm\n",
    "from plm_special.utils.console_logger import ConsoleLogger\n",
    "\n",
    "\n",
    "PLM_LAYER_SIZES = {\n",
    "    'gpt2': {\n",
    "        'base': 24,\n",
    "        'small': 12,\n",
    "        'large': 36,\n",
    "        'xl': 48\n",
    "    },\n",
    "    'llama': {\n",
    "        'base': 32,\n",
    "    },\n",
    "    't5-lm': { \n",
    "        'base': 12,\n",
    "        'small': 6,\n",
    "        'large': 24,\n",
    "        'xl': 24\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def save_model(args, model, save_dir):\n",
    "    if args.rank > 0:\n",
    "        # save lora weights\n",
    "        model.plm.save_pretrained(save_dir)\n",
    "        # save other modules except plm\n",
    "        torch.save(model.modules_except_plm.state_dict(), os.path.join(save_dir, 'modules_except_plm.bin'))\n",
    "    else:\n",
    "        # lora is disabled, save whole model\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, 'model.bin'))\n",
    "\n",
    "\n",
    "def load_model(args, model, model_dir):\n",
    "    if args.rank > 0:\n",
    "        # load lora weights\n",
    "        model.plm.load_adapter(model_dir, adapter_name='default')\n",
    "        # load other modules except plm\n",
    "        model.modules_except_plm.load_state_dict(torch.load(os.path.join(model_dir, 'modules_except_plm.bin')))\n",
    "    else:\n",
    "        # lora is disabled, load whole model\n",
    "        model.load_state_dict(torch.load(os.path.join(model_dir, 'model.bin')))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt/fine-tune model/train mdoel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt(args, model, exp_dataset, exp_dataset_info, eval_env_settings, checkpoint_dir, best_model_dir, eval_process_reward_fn):\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay,\n",
    "    )\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer,\n",
    "        lambda steps: min((steps + 1) / args.warmup_steps, 1)\n",
    "    )\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    trainer = Trainer(args, model=model, optimizer=optimizer, exp_dataset=exp_dataset, loss_fn=loss_fn, device=args.device, lr_scheduler=lr_scheduler, \n",
    "                      grad_accum_steps=args.grad_accum_steps)\n",
    "\n",
    "    target_return = exp_dataset_info.max_return * args.target_return_scale\n",
    "    best_eval_return = 0.\n",
    "\n",
    "    total_train_losses = []\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_logs, train_losses = trainer.train_epoch()\n",
    "        total_train_losses.extend(train_losses)\n",
    "        print('='* 20, f'Training Iteration #{epoch}', '=' * 20)\n",
    "        print('>' * 10, 'Training Information:')\n",
    "        pprint(train_logs)\n",
    "\n",
    "        if epoch % args.save_checkpoint_per_epoch == 0:  # save checkpoint\n",
    "            checkpoint_dir_epoch = os.path.join(checkpoint_dir, str(epoch))\n",
    "            if not os.path.exists(checkpoint_dir_epoch):\n",
    "                os.makedirs(checkpoint_dir_epoch)\n",
    "            save_model(args, model, checkpoint_dir_epoch)\n",
    "            print('Checkpoint saved at:', checkpoint_dir_epoch)\n",
    "\n",
    "        if epoch % args.eval_per_epoch == 0:\n",
    "            eval_logs = evaluate_on_env(args, env_settings=eval_env_settings, model=model, target_return=target_return, max_ep_num=args.trace_num,\n",
    "                                        process_reward_fn=eval_process_reward_fn)\n",
    "            episodes_return = eval_logs['episodes_return']\n",
    "            if best_eval_return < episodes_return:\n",
    "                best_eval_return = episodes_return\n",
    "                save_model(args, model, best_model_dir)\n",
    "                print('Best model saved at:', best_model_dir)\n",
    "\n",
    "            eval_logs['best_return'] = best_eval_return\n",
    "            print('>' * 10, 'Evaluation Information')\n",
    "            pprint(eval_logs)\n",
    "    # save training losses\n",
    "    train_losses_path = os.path.join(checkpoint_dir, 'train_losses.txt')\n",
    "    np.savetxt(train_losses_path, total_train_losses, fmt='%.6f', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, exp_dataset_info, env_settings, model_dir, result_dir, test_process_reward_fn):\n",
    "    model = load_model(args, model, model_dir)\n",
    "    print('Load model from:', model_dir)\n",
    "    target_return = exp_dataset_info.max_return * args.target_return_scale\n",
    "    results = test_on_env(args, model, result_dir, env_settings, target_return, args.trace_num, test_process_reward_fn, seed=args.seed)\n",
    "    print(results)\n",
    "    print('Test time:', results['time'], '\\nMean reward:', results['mean_reward'])\n",
    "    print('Results saved at:', result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperiencePool:\n",
    "    \"\"\"\n",
    "    Experience pool for collecting trajectories.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "    def add(self, state, action, reward, done):\n",
    "        \n",
    "        self.states.append(state)  # sometime state is also called obs (observation)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "    assert args.plm_type in cfg.plm_types\n",
    "    assert args.plm_size in cfg.plm_sizes\n",
    "    assert args.exp_pool_path is not None, 'please specify a experience pool path for training'\n",
    "    assert args.trace in cfg.trace_dirs.keys()\n",
    "    assert args.video in cfg.video_size_dirs.keys()\n",
    "\n",
    "    # 1. set seed\n",
    "    set_random_seed(args.seed)\n",
    "\n",
    "    # 2. create environment setting\n",
    "    trace_dir = cfg.trace_dirs[args.trace]\n",
    "    video_size_dir = cfg.video_size_dirs[args.video]\n",
    "    all_cooked_time ,all_cooked_bw ,all_file_names, all_mahimahi_ptrs = load_traces(trace_dir)\n",
    "    args.trace_num = min(args.trace_num, len(all_file_names))\n",
    "    if args.trace_num == -1:\n",
    "        args.trace_num = len(all_file_names)\n",
    "    if args.trace_num == len(all_file_names):\n",
    "        args.fixed_order = True\n",
    "\n",
    "    env_settings = {\n",
    "        'all_cooked_time': all_cooked_time,\n",
    "        'all_cooked_bw': all_cooked_bw,\n",
    "        'all_file_names': all_file_names,\n",
    "        'all_mahimahi_ptrs': all_mahimahi_ptrs,\n",
    "        'video_size_dir': video_size_dir,\n",
    "        'fixed': args.fixed_order,\n",
    "        'trace_num': args.trace_num,\n",
    "    }\n",
    "\n",
    "    # 3. create training dataset, fetch info\n",
    "    print(\"exp pool path\",args.exp_pool_path)\n",
    "    exp_pool = pickle.load(open(args.exp_pool_path, 'rb'))\n",
    "\n",
    "\n",
    "    # print(\"Type of exp_pool:\", type(exp_pool))\n",
    "    # print(\"states shape: \",np.array(exp_pool.states).shape)\n",
    "    # print(\"states[0].shape\",np.array(exp_pool.states[0]).shape)\n",
    "\n",
    "\n",
    "    # print(\"Type of states:\", type(exp_pool.states))\n",
    "    # print(\"Type of actions:\", type(exp_pool.actions))\n",
    "    # print(\"Type of rewards:\", type(exp_pool.rewards))\n",
    "    # print(\"Type of dones:\", type(exp_pool.dones))\n",
    "\n",
    "    # # Print the lengths of the lists stored in exp_pool\n",
    "    # print(\"Number of states:\", len(exp_pool.states))\n",
    "    # print(\"Number of actions:\", len(exp_pool.actions))\n",
    "    # print(\"Number of rewards:\", len(exp_pool.rewards))\n",
    "    # print(\"Number of dones:\", len(exp_pool.dones))\n",
    "\n",
    "    # print(\"1000th state:\", exp_pool.states[1000])\n",
    "    # print(\"1000th action:\",exp_pool.actions[1000])\n",
    "    # print(\"1000th reward:\",exp_pool.rewards[1000])\n",
    "    # print(\"1000th done:\", exp_pool.dones[1000])\n",
    "\n",
    "\n",
    "    exp_dataset = ExperienceDataset(exp_pool, gamma=args.gamma, scale=args.scale, max_length=args.w, sample_step=args.sample_step)\n",
    "    print(\"exp_dataset\",exp_dataset)\n",
    "    exp_dataset_info = Munch(exp_dataset.exp_dataset_info)\n",
    "    print('Experience dataset info:')\n",
    "    pprint(exp_dataset_info)\n",
    "    \n",
    "    # 4. create model\n",
    "    \n",
    "    # 4.1 load plm\n",
    "    # args.device_out and args.device_mid are used for model parallelism (currently only support llama) \n",
    "    # For data/modules near the input side, we use args.device.\n",
    "    # For data/modules near the output side, we use args.device_out.\n",
    "    # For data/modules lying in the middle, we use args.device_mid (it can be None). \n",
    "    # If args.device == args.device_out == args.device_mid (if not None), everything will be the same as using only one device.\n",
    "    plm, *_ = load_plm(args.plm_type, os.path.join(cfg.plm_dir, args.plm_type, args.plm_size), \n",
    "                       device_input_side=args.device, device_output_side=args.device_out, device_middle_side=args.device_mid)\n",
    "\n",
    "    if args.plm_type != 'llama':\n",
    "        plm = plm.to(args.device)\n",
    "    \n",
    "    if args.rank != -1:\n",
    "        plm = peft_model(plm, args.plm_type, rank=args.rank)\n",
    "\n",
    "    # 4.2 create state encoder\n",
    "    assert args.state_feature_dim is not None, 'please specify state feature dim to create state encoder'\n",
    "    state_encoder = EncoderNetwork(embed_dim=args.state_feature_dim)\n",
    "    state_encoder = state_encoder.to(args.device)\n",
    "\n",
    "    print(\"args.state_feature_dim: \",args.state_feature_dim)\n",
    "\n",
    "    # 4.3 create rl policy\n",
    "    plm_embed_size = cfg.plm_embed_sizes[args.plm_type][args.plm_size]\n",
    "    max_ep_len = exp_dataset_info.max_timestep + 1\n",
    "    rl_policy = OfflineRLPolicy(state_feature_dim=args.state_feature_dim, bitrate_levels=BITRATE_LEVELS, state_encoder=state_encoder, plm=plm, plm_embed_size=plm_embed_size, \n",
    "                                           max_length=args.w, max_ep_len=max_ep_len, device=args.device, device_out=args.device_out, which_layer=args.which_layer)\n",
    "\n",
    "    # 5. handling directory and path\n",
    "\n",
    "    # extract training experience pool information\n",
    "    train_exp_pool_info = args.exp_pool_path.split('/')[-4:-1]\n",
    "    train_exp_pool_info = '_'.join(train_exp_pool_info)\n",
    "    models_dir = os.path.join(cfg.plm_ft_dir, f'{args.plm_type}_{args.plm_size}', train_exp_pool_info + f'_ss_{args.sample_step}', f'rank_{args.rank}_w_{args.w}_gamma_{args.gamma}_sfd_{args.state_feature_dim}'\\\n",
    "                              f'_lr_{args.lr}_wd_{args.weight_decay}_warm_{args.warmup_steps}_epochs_{args.num_epochs}_seed_{args.seed}')\n",
    "    results_dir = os.path.join(cfg.results_dir, f'{args.trace}_{args.video}', f'trace_num_{args.trace_num}_fixed_{args.fixed_order}', f'{args.plm_type}_{args.plm_size}',\n",
    "                               f'early_stop_{args.which_layer}_rank_{args.rank}_w_{args.w}_gamma_{args.gamma}_tgt_scale_{args.target_return_scale}_seed_{args.seed}')\n",
    "    checkpoint_dir = os.path.join(models_dir, f'early_stop_{args.which_layer}_checkpoint')\n",
    "    best_model_dir = os.path.join(models_dir, f'early_stop_{args.which_layer}_best_model')\n",
    "\n",
    "\n",
    "    # 6. start training/testing\n",
    "    def process_reward(reward, \n",
    "                       max_reward=exp_dataset_info.max_reward, \n",
    "                       min_reward=exp_dataset_info.min_reward, \n",
    "                       scale=args.scale):\n",
    "        reward = min(max_reward, max(min_reward, reward))  # bound reward\n",
    "        return (reward - min_reward) / (max_reward - min_reward) / scale\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    if args.adapt:\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        if not os.path.exists(best_model_dir):\n",
    "            os.makedirs(best_model_dir)\n",
    "        console_log = open(os.path.join(models_dir, f'early_stop_{args.which_layer}_console.log'), 'w')\n",
    "        sys.stdout = ConsoleLogger(sys.__stdout__, console_log)\n",
    "        adapt(args, rl_policy, exp_dataset, exp_dataset_info, env_settings, checkpoint_dir, best_model_dir, process_reward)\n",
    "    if args.test:\n",
    "        if not os.path.exists(results_dir):\n",
    "            os.makedirs(results_dir)\n",
    "        model_dir = args.model_dir if args.model_dir is not None else best_model_dir\n",
    "        assert os.path.exists(model_dir), f'Model weight dir {model_dir} does not exist.'\n",
    "        test(args, rl_policy, exp_dataset_info, env_settings, model_dir, results_dir, process_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocation successful!\n",
      "Arguments:\n",
      "Namespace(adapt=True, device='cpu', device_mid=None, device_out='cpu', eval_per_epoch=1, exp_pool_path='artifacts/exp_pools/exp_pool_l4s.pkl', fixed_order=True, gamma=1.0, grad_accum_steps=32, lr=0.0001, model_dir=None, num_epochs=1, plm_size='base', plm_type='llama', rank=128, sample_step=10, save_checkpoint_per_epoch=1, scale=1000, seed=100003, state_feature_dim=256, target_return_scale=1.0, test=False, trace='fcc-test', trace_num=100, video='video1', w=20, warmup_steps=2000, weight_decay=0.0001, which_layer=-1)\n",
      "Loading traces from data/traces/test/fcc-test/\n",
      "all_mahimahi_ptrs:  [28, 99, 32, 27, 26, 30, 35, 93, 187, 294, 164, 163, 64, 38, 59, 278, 40, 52, 15, 51, 55, 190, 29, 41, 54, 125, 16, 273, 37, 205, 141, 27, 145, 198, 46, 45, 26, 287, 51, 32, 62, 284, 22, 47, 20, 172, 32, 26, 49, 35, 59, 144, 275, 30, 64, 5, 15, 263, 19, 54, 28, 35, 56, 62, 41, 43, 165, 210, 50, 54, 28, 5, 171, 288, 55, 9, 44, 47, 54, 281, 272, 215, 64, 277, 61, 114, 13, 196, 40, 158, 16, 23, 143, 303, 224, 29, 26, 205, 55, 3]\n",
      "exp pool path artifacts/exp_pools/exp_pool_l4s.pkl\n",
      "exp_dataset <plm_special.data.dataset.ExperienceDataset object at 0x0000017EC8074520>\n",
      "Experience dataset info:\n",
      "{'max_action': 3,\n",
      " 'max_return': 97.36226315790829,\n",
      " 'max_reward': 380000,\n",
      " 'max_timestep': 721857,\n",
      " 'min_action': 1,\n",
      " 'min_return': 2.631578947368421e-05,\n",
      " 'min_reward': 0,\n",
      " 'min_timestep': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:41<00:00, 50.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If tokenizer is loaded:  [1, 22172, 3186] \n",
      "\n",
      "args.state_feature_dim:  256\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 11826937856 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 90\u001b[0m\n\u001b[0;32m     87\u001b[0m     run(args)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 90\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 87\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(args)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Call the run function with the initialized arguments\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 91\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     89\u001b[0m plm_embed_size \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mplm_embed_sizes[args\u001b[38;5;241m.\u001b[39mplm_type][args\u001b[38;5;241m.\u001b[39mplm_size]\n\u001b[0;32m     90\u001b[0m max_ep_len \u001b[38;5;241m=\u001b[39m exp_dataset_info\u001b[38;5;241m.\u001b[39mmax_timestep \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 91\u001b[0m rl_policy \u001b[38;5;241m=\u001b[39m \u001b[43mOfflineRLPolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_feature_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_feature_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbitrate_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBITRATE_LEVELS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplm_embed_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplm_embed_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ep_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_ep_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhich_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhich_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# 5. handling directory and path\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# extract training experience pool information\u001b[39;00m\n\u001b[0;32m     97\u001b[0m train_exp_pool_info \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mexp_pool_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\deols\\OneDrive\\Documents\\GitHub\\NetLLM-Personal\\adaptive_bitrate_streaming\\plm_special\\models\\rl_policy.py:44\u001b[0m, in \u001b[0;36mOfflineRLPolicy.__init__\u001b[1;34m(self, state_feature_dim, bitrate_levels, state_encoder, plm, plm_embed_size, max_length, max_ep_len, device, device_out, residual, conv_size, which_layer, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_encoder \u001b[38;5;241m=\u001b[39m state_encoder\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_feature_dim \u001b[38;5;241m=\u001b[39m state_feature_dim\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_timestep \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_ep_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplm_embed_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_return \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1\u001b[39m, plm_embed_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_action \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1\u001b[39m, plm_embed_size)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\deols\\anaconda3\\envs\\urllc\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:142\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq \u001b[38;5;241m=\u001b[39m scale_grad_by_freq\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    143\u001b[0m                             requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m _freeze)\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_parameters()\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 11826937856 bytes."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from argparse import Namespace\n",
    "\n",
    "def main():\n",
    "    # Set the per-process memory fraction and empty cache\n",
    "    torch.cuda.set_per_process_memory_fraction(0.5, 0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Test memory allocation\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    try:\n",
    "        x = torch.randn(10000, 10000, device=device)\n",
    "        print(\"Memory allocation successful!\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Memory allocation failed: {e}\")\n",
    "\n",
    "    # # Initialize arguments directly\n",
    "    # args = Namespace(\n",
    "    #     exp_pool_path='artifacts/exp_pools/exp_pool.pkl',\n",
    "    #     sample_step=10,\n",
    "    #     trace='fcc-test',\n",
    "    #     trace_num=100,\n",
    "    #     video='video1',\n",
    "    #     fixed_order=True,\n",
    "    #     plm_type='llama',\n",
    "    #     plm_size='base',\n",
    "    #     rank=128,\n",
    "    #     state_feature_dim=256,\n",
    "    #     w=20,\n",
    "    #     gamma=1.0,\n",
    "    #     lr=1e-4,\n",
    "    #     weight_decay=1e-4,\n",
    "    #     warmup_steps=2000,\n",
    "    #     num_epochs=1,\n",
    "    #     eval_per_epoch=1,\n",
    "    #     save_checkpoint_per_epoch=1,\n",
    "    #     target_return_scale=1.0,\n",
    "    #     which_layer=-1,\n",
    "    #     adapt=True,\n",
    "    #     test=True,\n",
    "    #     grad_accum_steps=32,\n",
    "    #     seed=100003,\n",
    "    #     scale=1000,\n",
    "    #     model_dir=None,\n",
    "    #     device='cuda:0',\n",
    "    #     device_out='cuda:0',\n",
    "    #     device_mid=None\n",
    "    # )\n",
    "    # Initialize arguments directly\n",
    "    args = Namespace(\n",
    "        exp_pool_path='artifacts/exp_pools/exp_pool_l4s.pkl',\n",
    "        sample_step=10,\n",
    "        trace='fcc-test',\n",
    "        trace_num=100,\n",
    "        video='video1',\n",
    "        fixed_order=True,\n",
    "        plm_type='llama',\n",
    "        plm_size='base',\n",
    "        rank=128,\n",
    "        state_feature_dim=256,\n",
    "        w=20,\n",
    "        gamma=1.0,\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-4,\n",
    "        warmup_steps=2000,\n",
    "        num_epochs=1,\n",
    "        eval_per_epoch=1,\n",
    "        save_checkpoint_per_epoch=1,\n",
    "        target_return_scale=1.0,\n",
    "        which_layer=-1,\n",
    "        adapt=True,\n",
    "        test=False,\n",
    "        grad_accum_steps=32,\n",
    "        seed=100003,\n",
    "        scale=1000,\n",
    "        model_dir=None,\n",
    "        device='cpu',\n",
    "        device_out='cpu',\n",
    "        device_mid=None\n",
    "    )\n",
    "\n",
    "    # Print the initialized arguments\n",
    "    print('Arguments:')\n",
    "    print(args)\n",
    "\n",
    "    # Call the run function with the initialized arguments\n",
    "    run(args)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urllc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
